              <div class="masonry-item col-md-12">
                <div class="bgc-white p-20 bd">
                  <h6 class="c-grey-900">Pre-Trained Language Models</h6>
                  <div class="mT-30">
                    
<p><b>Annotation models for lemma, UPOS, XPOS and dependency parsing (where supported) trained on RRT UD 2.7.</b></p>
<ul>
   <li>Stanza <a href="resources/models/ud27_stanza.zip">Download (756Mb)</a></li>
   <li>RNNTagger <a href="resources/models/ud27_rnntagger.zip">Download (668Mb)</a></li>
   <li>NLP-Cube <a href="resources/models/ud27_nlpcube.zip">Download (345Mb)</a></li>
   <li>UDPipe <a href="resources/models/ud27_udpipe.zip">Download (13Mb)</a></li>
   <li>TreeTagger <a href="resources/models/ud27_treetagger.zip">Download (1.4Mb)</a></li>
   <li>Scripts used in training and evaluating the models are available in our GitHub <a href="https://github.com/racai-ai/RoBLARK_evaluation" target="_blank">here</a>.</li>
   <li>A working version of the TTL tool is available in the <a href="https://github.com/racai-ai/TEPROLIN" target="_blank">TEPROLIN service repository</a>.</li> 
   <li>For downloading the corpus visit the <a href="https://universaldependencies.org/" target="_blank">Universal Dependencies website</a> or directly download UD 2.7 treebanks from <a href="http://hdl.handle.net/11234/1-3424" target="_blank">http://hdl.handle.net/11234/1-3424</a>.</li> 
</ul>   
<br/><br/>

<p><b>Classification models</b></p>
<ul>
    <li>PyEuroVoc - Classification of legal documents using EuroVoc descriptors, based on BERT models, for 22 languages 
        (Bulgarian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Greek, Hungarian, 
        Italian, Latvian, Lithuanian, Maltese, Polish, Portuguese, Romanian, Spanish, Slovak, Slovene, Swedish).
         A GitHub repo with scripts and example usage is available 
        <a href="https://github.com/racai-ai/pyeurovoc" target="_blank">here</a>. Related paper is 
        <i>Avram Andrei-Marius, Vasile Pai?, and Dan Tufi?. "PyEuroVoc: A Tool for Multilingual Legal Document Classification with EuroVoc Descriptors."  In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021), 2021.</i>
    </li>
    <li>FastText EuroVoc classification models, based on Common Crawl FastText embeddings for most languages and CoRoLa embeddings for Romanian. Models available for 
        multiple languages can be downloaded <a href="https://relate.racai.ro/resources/EUROVOC/">here</a>. A modified FastText application allowing models to be
        interrogated online is available <a href="https://github.com/racai-ai/ServerFastText" target="_blank">here</a>.
    </li> 
</ul>   
<br/><br/>


<p><b>Contextualized embeddings</b></p>
<ul>
    <li>RoBERT: There are two models available  
        <a href="https://huggingface.co/dumitrescustefan/bert-base-romanian-cased-v1" target="_blank">bert-base-romanian-cased-v1</a> and
        <a href="https://huggingface.co/dumitrescustefan/bert-base-romanian-uncased-v1" target="_blank">bert-base-romanian-uncased-v1</a> .
        A GitHub repo with useful scripts is available <a href="https://github.com/dumitrescustefan/Romanian-Transformers" target="_blank">here</a> .
        Related paper is <i>Dumitrescu Stefan, Andrei-Marius Avram, and Sampo Pyysalo. "The birth of Romanian BERT." In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pp. 4324-4328, 2020</i>.
    </li>
    <li>Romanian DistilBERT: Constructed based on the bert-base-romanian-cased-v1 model, the model is available on HuggingFace as 
        <a href="https://huggingface.co/racai/distilbert-base-romanian-cased" target="_blank">distilbert-base-romanian-cased</a>.
        A GitHub repo is available <a href="https://github.com/racai-ai/Romanian-DistilBERT">here</a>.
    </li> 
        
</ul>   
<br/><br/>


<p><b>Word Embeddings from the CoRoLa project</b></p>
<ul>
    <li>All word embeddings from the CoRoLa project can be downloaded and used interactively <a href="http://corolaws.racai.ro/word_embeddings/" target="_blank">here</a>.</li>
    <li>The recommended model, according to a number of experiments, can be downloaded directly from <a href="http://corolaws.racai.ro/word_embeddings/we/corola.300.20.vec.zip">here</a>.</li>
</ul>


                  </div>
                </div>
              </div>
